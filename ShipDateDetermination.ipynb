{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "447fd5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Config (update these variables based on your GCP project and naming conventions)\n",
    "\n",
    "GCP_PROJECT_ID           = 'GCPProject1'        # Google Cloud Project ID\n",
    "BQ_DATASET               = 'OrderData'      # Name of BigQuery Dataset to create (if it does not exist)\n",
    "BQ_MODEL_NAME            = 'shipDateDeterminationModel' # Name of BigQuery model that will be trained\n",
    "GCS_LOCATION             = 'us-central1'    # Google Cloud Storage Location\n",
    "GCS_BUCKET               = 'gs://aiplatformmodelregistry/ShipDateModel/'\n",
    "VERTEX_LOCATION          = 'us-central1'\n",
    "VERTEX_MODEL_NAME        = 'shipDateDeterminationModel'\n",
    "VERTEX_SERVING_CONTAINER = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-1:latest' # https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b771c371",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-aiplatform==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a1710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de56ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BQ client\n",
    "try:\n",
    "    bq_client = bigquery.Client()\n",
    "except Exception as e: \n",
    "    print('[ EXCEPTION ] {}'.format(e))\n",
    "\n",
    "QUERY = f'''\n",
    "CREATE OR REPLACE MODEL\n",
    "  `{GCP_PROJECT_ID}.{BQ_DATASET}.{BQ_MODEL_NAME}` OPTIONS(model_type='AUTOML_REGRESSOR',\n",
    "               input_label_cols=['FULFILLMENT_DATE_IN_HOURS'],\n",
    "               budget_hours=5.0,\n",
    "               OPTIMIZATION_OBJECTIVE='MINIMIZE_RMSE')\n",
    "AS\n",
    "SELECT * \n",
    "FROM `ma-rd-devils-sren.OrderData.ProcessedOrderDetails`\n",
    "'''\n",
    "query_job = bq_client.query(QUERY)\n",
    "rows = query_job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d906bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export BQML Model to the newly created Cloud Storage Bucket\n",
    "bq_client = bigquery.Client()\n",
    "QUERY = f'''\n",
    "EXPORT MODEL `{GCP_PROJECT_ID}.{BQ_DATASET}.{BQ_MODEL_NAME}`\n",
    "OPTIONS(URI = '{GCS_BUCKET}')\n",
    "'''\n",
    "print(QUERY)\n",
    "query_job = bq_client.query(QUERY)\n",
    "rows = query_job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67877e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/1018321702381/locations/us-central1/models/8344615955657129984/operations/1628306626869985280\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/1018321702381/locations/us-central1/models/8344615955657129984\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/1018321702381/locations/us-central1/models/8344615955657129984')\n"
     ]
    }
   ],
   "source": [
    "# Pre-built Model Serving Containers:\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n",
    "\n",
    "aiplatform.init(project=GCP_PROJECT_ID, location=VERTEX_LOCATION)\n",
    "    \n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=VERTEX_MODEL_NAME,\n",
    "    artifact_uri=f'gs://aiplatformmodelregistry/ShipDateModel/',\n",
    "    serving_container_image_uri=VERTEX_SERVING_CONTAINER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b463729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/1018321702381/locations/us-central1/endpoints/4373329489711595520/operations/2562803549549363200\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/1018321702381/locations/us-central1/endpoints/4373329489711595520\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/1018321702381/locations/us-central1/endpoints/4373329489711595520')\n",
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/1018321702381/locations/us-central1/endpoints/4373329489711595520\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/1018321702381/locations/us-central1/endpoints/4373329489711595520/operations/6239992645297373184\n"
     ]
    },
    {
     "ename": "FailedPrecondition",
     "evalue": "400 Error: model server never became ready. Please validate that your model file or container configuration are valid. Model server logs can be found at https://console.cloud.google.com/logs/viewer?project=1018321702381&resource=aiplatform.googleapis.com%252FEndpoint&advancedFilter=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%0Aresource.labels.endpoint_id%3D%224373329489711595520%22%0Aresource.labels.location%3D%22us-central1%22.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPrecondition\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-48f55ae204e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmachine_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"n1-highcpu-4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmin_replica_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmax_replica_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata, encryption_spec_key_name, sync)\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mencryption_spec_key_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencryption_spec_key_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencryption_spec_key_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m             \u001b[0msync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1660\u001b[0m         )\n\u001b[1;32m   1661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36m_deploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata, encryption_spec_key_name, sync)\u001b[0m\n\u001b[1;32m   1793\u001b[0m             \u001b[0mexplanation_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplanation_metadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m             \u001b[0mexplanation_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplanation_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1795\u001b[0;31m             \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1796\u001b[0m         )\n\u001b[1;32m   1797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36m_deploy_call\u001b[0;34m(cls, api_client, endpoint_resource_name, model_resource_name, endpoint_resource_traffic_split, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata)\u001b[0m\n\u001b[1;32m    876\u001b[0m         )\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m         \u001b[0moperation_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m     def undeploy(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# Pylint doesn't recognize that this is valid in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPrecondition\u001b[0m: 400 Error: model server never became ready. Please validate that your model file or container configuration are valid. Model server logs can be found at https://console.cloud.google.com/logs/viewer?project=1018321702381&resource=aiplatform.googleapis.com%252FEndpoint&advancedFilter=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%0Aresource.labels.endpoint_id%3D%224373329489711595520%22%0Aresource.labels.location%3D%22us-central1%22."
     ]
    }
   ],
   "source": [
    "# Deploy Vertex Model as an Endpoint\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=f'''{VERTEX_MODEL_NAME}-endpoint''',\n",
    "    machine_type=\"n1-highcpu-4\",\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m71"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
